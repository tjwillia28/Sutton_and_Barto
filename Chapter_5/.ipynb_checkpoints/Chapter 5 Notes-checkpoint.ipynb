{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here we do not assume complete knowledge of the environment\n",
    "- Monte Carlo methods require only experience -sample sequences of states, acitons and rewards from actual or simulated interaction with an environment.  Learning from actual experience is striking because it requires no prior knowledge of the environment's dynamics, yet can still attain oprimal behavior.\n",
    "- Learning from simulated experience is also powerful although it requires a model\n",
    "    - The model only nees to generate sample transitions, not the complete probability distributions of all possible transitions that is required for DP.  \n",
    "    - In surprisingly many cases it is easy to generate experience sampled according to the desired probability distribution but infeasible to obtain the distributions in explicit form.\n",
    "- Monte Carlo methods are ways of solving the RL problem based on averaging sample returns.  \n",
    "- To ensure that well-defined returns are available, here we define MC methods only for episodic tasks.\n",
    "- Only on the completion of an episode are value estimates and policies changed.\n",
    "    - MC methods can thus be incrmemental in an episode-by-episode sense, but not in a step-by-step (online) sense\n",
    "- Whereas in DP we computed value functions from knowledge of the MDP, here we learn value functions from sample returns with the MDP.  The value functions and corresponding policies still interact to attain optimality in essentially the same way (GPI)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1: Monte Carlo Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We begin by considering MC methods for learning the state-value function for a given policy\n",
    "- Recall that the value of a state is the expected return- expected cumulative future discounted reward- starting from that state.  An obvious way to esitmate it from experience, then, is simply to average the returns observed after visits to that state.  As more returns are observed, the averge should converge to the expected value.  This idea undlies all MC methods\n",
    "- Suppose we wish to estimate $v_{\\pi}(s)$ given a set of episodes obtained by following $\\pi$ and passing through s.\n",
    "- Each occurrence of stae s in an episode is called a visit to s\n",
    "    - The first time a state is visited in an episode is referred to as the first visit.\n",
    "- The first-visit MC method estimates $v_{\\pi}(s)$ as the average of the returns following first visits to s, whereas the every-visit MC method averages the returns following all visits to s\n",
    "    - We will focus on first-visit MC in this chapter\n",
    "- Both first-visit MC and every-visit MC converge to $v_{\\pi}(s)$ as the number of visits (or first visits) to s -> inf\n",
    "- In this case, each return is an independent, identically distributed estimate of $v_{\\pi}(s)$ with finite variance\n",
    "- The ability of MC methods to work with sample episodes alone can be a significant advantage even when one has complete knowledge of the environment's dynamics\n",
    "- The backup diagram for MC estimation of $v_{\\pi}(s)$ has the state node at the root and below all the sampled transitions until the end of the episode.\n",
    "- An important face about MC methods is that the estimates for each state are independent.  The estimate for one state does not build upon the estimate of any other state, as in the case in DP.  In other words, MC methods do not bootstrap\n",
    "- Note that the computational expense of estimating the value of a single state is independent of the number of states.\n",
    "- If one is interested in only the value at one point, or any fixed small set of points, then this MC method can be far more efficient than the iteraive method based on local consistency.\n",
    "- Three advantages of MC methods over DP methods:\n",
    "    - ability to learn from actual experience\n",
    "    - ability to learn from simulated experience\n",
    "    - estimating value of state is independent of total number of states, which is attractive when we only care about a subset of states "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSEUDOCODE: First visit MC prediction, for estimating V ~= V_PI\n",
    "'''\n",
    "Input: a policy PI to be evaluated\n",
    "\n",
    "Initialize:\n",
    "    V(s) in R, arbitrarily, for all s in S\n",
    "    Returns(s) <- an empty list, for all s in S\n",
    "\n",
    "Loop forever (for each episode):\n",
    "    Generate an episode following PI: S_0, A_0, R_1, S_1, A_1, R_2, ..., S_(T-1), A_(T-1), R_T\n",
    "    G <- 0\n",
    "    Loop for each step of episode, t = T-1, T-2, ..., 0:\n",
    "        G <- gamma*G + R_(t+1)\n",
    "        Unless S_t appears in S_0, S_1, ..., S_(t-1):\n",
    "            Append G to Returns(S_t)\n",
    "            V(S_t) <- average(Returns(S_t))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2: MC Estimation of Action Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
