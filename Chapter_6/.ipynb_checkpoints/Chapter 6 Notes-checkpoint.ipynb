{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Temporal-Difference Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TD learning is a combination of MC ideas and DP ideas\n",
    "    - Like MC methods, TD methods can learn directly from raw experience without a model of the environment's dynamics.\n",
    "    - Like DP, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome (i.e. they bootstrap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1 TD Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Like MC methods, TD methods use experience to solve the prediction problem(i.e. estimate $v_{\\pi}$ given policy $\\pi$)\n",
    "- While MC methods must wait until the end of the episode to determine the increment to $V(S_{t})$, TD methods need to only wait until the next time step\n",
    "    - At time $t+1$ they immediately form a target and make a useful update using the observed reward $R_{t+1}$ and the estimate $V(S_{t+1})$.  The simplest TD method makes the update immediately on transition to $S_{t+1}$ and receiving $R_{t+1}$: $$V(S_{t}) \\leftarrow V(S_{t}) + \\alpha [R_{t+1} + \\gamma V(S_{t+1}) - V(S_{t})]$$ \n",
    "    - In effect, the target for the MC update is $G_{t}$, whereas the target for the TD update is $R_{t+1} + \\gamma V(S_{t+1})$ \n",
    "    - This TD mehtod is called TD(0) or one-step TD because it is a special case of the TD(lambda) and n-step TD methods\n",
    "- TD methods combine the sampling of MC with the bootstrapping of DP\n",
    "- We refer to TD and MC updates as sample updates because they involve looking ahead to a sample successor state (or state-action pair), using the value the value of the successor state and the reward along the way to compute a backed-up value, and then updating the value of the original state( or state-action pair) accordingly.\n",
    "- Sample updates differ from the expected updates of DP methods in that they are based on a single sample successor rather than on a complete distribution of all possible successors\n",
    "- Also note that the quantity in brackets in the TD(0) update is a sort of error, measuring the difference between the estimated value of $S_{t}$ and the better estimate $R_{t+1} + \\gamma V(S_{t+1})$. We call this quantitiy the TD error: $$\\delta_{t} \\doteq R_{t+1} + \\gamma V(S_{t+1}) - V(S_{t})$$\n",
    "- Each error is proportional to the change over time of the prediction, that is, the temporal differences in predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2 Advantages of TD Prediction Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TD methods have an advantage over DP methods because they do NOT require a model of the environment, of its reward and next-state probability distributions\n",
    "- Another advantage of TD methods over MC methods is that they are naturally implemented in an online, fully incremental fashion.\n",
    "    - MC methods need to wait until the end of an episode while TD methods only need to wait one wime setp\n",
    "- For any fixed policy $PI$, TD(0) has been proved to converge to $v_{\\pi}$, in the mean for a constant step-size parameter if it is sufficiently small, and with probability q if the step-size paramater decreases according to the usual stochastic approximation conditions.\n",
    "- In practice, TD methods have usually been found to converge faster than constant-alpha MC methods on stochastic tasks (although this has not been proven mathematically and is an open question in the RL community)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3 Optimality of TD(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Suppose there is available only a finite amount of experience, ay 10 episdoes or 100 time steps. In this case, a common approach with incremental learning methods is to persent te experience repeatedly until the method converges upon an answer.  Given an approximate value function, V, the increments specified by the TD error are computed for every time step t at which a non terminal state is visited, but the value function is changed only once, by the sum of all the increments.  Then all the available experience is processed again the the new value fxn to produce a new overall increment , and so on, until the value fxn converges.  We call this batch updating because updates are made only after processing each complete batch of training data.\n",
    "- Batch MC methods always find the estimates that minimize mean-squared error on the training set, wereas batch TD(0) always find the estimates that would be exactly correct for the maximum-likelihood model of the Markov process\n",
    "    - In general, the maximum-likelihood estimate of a parameter is the paramater value whose probability of generating the data is greatest\n",
    "    - In this case, the maximum-likelihood estimate is the model of the Markov process formed in the obvious way from the observed episodes: the estimated transition probability from i to j is the fraction of observerd transitions from i that went to j, and the associated expected reward is the average of the rewards observed on those transitions.\n",
    "    - Given this model, we can compute the estimate of the value fxn that would be exactly correct if the model were exactly correct.  This is called the certainty-equivalence estimate because it is equivalent to assuming that the estimate of the underlying process was known with certainty rather than being approximated.  In general, batch TD(0) converges to the certainty-equivalence estimate\n",
    "- Although the nonbatch methods do not achieve either the certainty-equivalence or the minimum squared-error estimate, they can be understood as moving roughly in these directions.\n",
    "- Finally, it is worth noting that although the certainty-equivalence estimate is in some sense an optimal solution, it is almost never feasible to compute it directly(it is too computationally expensive as the state space grows)\n",
    "    - It is striking that TD methods can approximate the same solution using memory no more than order n and repeated computations over the training set.\n",
    "    - On tasks with large state-spaces, TD methods may be the only feasible way of approximating the certainty-equivalence solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4     Sarsa: On-policy TD control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For an on-policy method we must estimate $q_{\\pi}(S,a)$ for the current behavior policy $\\pi$ and for all states s and actions a.  This can be done using essentially the same method we used before for learning $v_{\\pi}$\n",
    "- Now we consider the transitions from state-action pair to state-action pair, and learn the values of state-action pairs.\n",
    "- The theorems assuring convergence of state values under TD(0) also apply to the corresponding algorithm for action values: \n",
    "$$Q(S_{t}, A_{t}) \\leftarrow Q(S_{t}, A_{t}) + \\alpha[R_{t+1} + \\gamma Q(S_{t+1}. A_{t+1}) - Q(S_{t}, A_{t})]$$\n",
    "    - This update is done after every transistion from a nonterminal state $S_{t}$\n",
    "- As in all on-policy methods, we continually estimate $q_{\\pi}$ for the behavior policy $\\pi$, and at the same time change $\\pi$ toward greediness w.r.t. $q_{\\pi}$\n",
    "- Sarsa converges with probability 1 to an optimal policy and action-value fxn as long as all state-action pairs are visited an infinite number of times and the policy converges in the limit to the greedy policy (which can be arranged with $\\epsilon$-greedy policies by setting $\\epsilon$ = 1/t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nAlgorithm parameters: step size alpha in (0,1]. small eps > 0\\nInitialize Q(s,a), for all s in S+, a in A(s), arbitrarily except that Q(terminal,.) = 0\\n\\nLoop for each episode:\\n    Initialize S\\n    Choose A from S using policy derived from Q (e.g. eps-greedy)\\n    Loop for each step of episode:\\n        Take action A, observe R, S'\\n        Choose A' and S' using policy derived from Q (e.g. eps-greedy)\\n        Q(S,A) <- Q(S,A) + alpha[R + gamma*Q(S', A') - Q(S,A)]\\n        S <- S'; A <- A';\\n    until S is terminal\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PSEUDOCODE: Sarsa for estimating Q ~= q_*\n",
    "\"\"\"\n",
    "Algorithm parameters: step size alpha in (0,1]. small eps > 0\n",
    "Initialize Q(s,a), for all s in S+, a in A(s), arbitrarily except that Q(terminal,.) = 0\n",
    "\n",
    "Loop for each episode:\n",
    "    Initialize S\n",
    "    Choose A from S using policy derived from Q (e.g. eps-greedy)\n",
    "    Loop for each step of episode:\n",
    "        Take action A, observe R, S'\n",
    "        Choose A' and S' using policy derived from Q (e.g. eps-greedy)\n",
    "        Q(S,A) <- Q(S,A) + alpha[R + gamma*Q(S', A') - Q(S,A)]\n",
    "        S <- S'; A <- A';\n",
    "    until S is terminal\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.5 Q-learning: Off-policy TD Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q-learning is defined as:\n",
    "$$ Q(S_{t}, A_{t}) \\leftarrow Q(S_{t}, A_{t}) +\\alpha[R_{t+1} + \\gamma max_{a} Q(S_{t+1}, a) - Q(S_{t}, A_{t})]$$\n",
    "- In this case, the learned action-value fxn, Q, directly approximates $q_{*}$, the optimal action-value function, independent of the policy being followed\n",
    "- Windy Gridworld Example:\n",
    "    - Q-learning learns the optimal policy but does not take into consideration action selection(i.e. eps-greedy)\n",
    "    - Sarsa on the other hand takes action selection into account\n",
    "    - Although Q-learning actually learns the values of the optimal policy, its online performace is worse than that of Sarsa\n",
    "    - If we decay epsioln then both methods would asymptotically converge to the optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSEUDOCODE: Q-Learning for estimating PI ~= PI_*\n",
    "\"\"\"\n",
    "Algorithm parameters: step size alpha in (0,1], small epsilon > 0\n",
    "Initialize Q(s,a), for all s in S+, a in A(s), arbitrarily except that Q(terminal,.) = 0\n",
    "\n",
    "Loop for each episode\n",
    "    Initialize S\n",
    "    Loop for each step of episode:\n",
    "        Choose A from S using policy derived from Q (e.g. eps-greedy)\n",
    "        Take action A, observe R, S'\n",
    "        Q(S,A) <- Q(S,A) + alpha[R + gamma*max_aQ(S',a) - Q(S,A)]\n",
    "        S <- S'\n",
    "    until S is terminal\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.6 Expected Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Expected Sarsa uses the following update:\n",
    "    $$Q(S_{t}, A_{t}) \\leftarrow Q(S_{t}, A_{t} + \\alpha[R_{t+1} + \\gamma E_{\\pi}[Q(S_{t+1}, A_{t+!}) | S_{t+1}] - Q(S_{t},A_{t})]$$ <br> $$\\leftarrow Q(S_{t}, A_{t}) + \\alpha[R_{t+1} + \\gamma \\sum\\limits_{a} \\pi(a|S_{t+1})(Q(S_{t+1}, a) - Q(S_{t}, A_{t}]$$\n",
    "- Given the next state, $S_{t+1}$, this algorithm moves deterministically in the same direction as Sarsa moves in expectation\n",
    "- Expected Sarsa is more complex computationally than Sarsa but, in return, it eliminates the variance due to the random selection of $A_{t+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.7 Maximization Bias and Double Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All the control algorithms that we have discussed so far involve maximization in the construction of their target policies\n",
    "    - Q-learning: the target policy is the greedy policy given the current action values, which is defined with a max\n",
    "    - Sarsa: the policy is often epsilon-greed, which also invlolves a maximization operation\n",
    "- In these algorithms (Q-learning and Sarsa), a maximium over estimates can lead to a significant bias.\n",
    "    - Explanation: Consider a single state s where are many actions a whise true values, q(s,a), are all zeros but whose estimated values, Q(s,a), are uncertain and this distributed some above and some below 0.  The maximum of the true values is zero, but the maximum of the estimates is positive, a positive bias.  We call this maximization bias.\n",
    "- Maximization Bias: when the maximum of estimates is larger than the maximum of the true value\n",
    "    - There will be a positive maxmimum bias if we use the maximum of the estimates as an estimate of the maximum of true values\n",
    "- One way to view the problem is that it is due to using the same samples(plays) both to determine the maximization action and to estimate its value.  \n",
    "- Suppose we divided the plays in two sets and used them to learn two independent estimates, call them $Q_{1}(a)$ and $Q_{2}(a)$, each an estimate of the true value $q(a)$, for all a in A.  We could then use one estimate, say $Q_{1}$, to determine the maximizing action $A^{*} = argmax_{a}Q_{1}(a)$ and the other, $Q_{2}$, to provide the estimate of its value,  $Q_{2}(A^{*}) = Q_{2}(argmax_{a}Q_{1}(a))$.  This estimate will then be unbiased in the sense that $E[Q_{2}(A^{*})] = q(A^{*})$.  This is the idea of double learning.  Note that although we learn two estimates, only one estimate is updated on each play; double learning doubles the memory requirements , but does not increase the amount of computation per step.\n",
    "- The double learning analagous to Q-learning, called Double Q-learning, divides the time steps in two, perhaps by filling a coin on each step.  If the coin comes up heads, the update is:\n",
    "$$ Q_{1}(S_{t}, A_{t}) \\leftarrow Q_{1}(S_{t}, A_{t}) + \\alpha[R_{t+1} + \\gamma Q_{2}(S_{t+1}, argmax_{a}Q_{1}(S_{t+1}, a)) - Q_{1}(S_{t}, A_{t})]$$\n",
    "- If the coin comes up tails, then the same update is done with $Q_{1}$ and $Q_{2}$ switched, so tthat Q_2 is updated.  \n",
    "- The two approximate value fxns are treated completely symmetrically.  - The behavior policy can use both action-value estimates.\n",
    "    - For example, an eps-greedy policy for Double Q-learning could be based on the average (or sum) of the two action-value estimates\n",
    "- There are also double versions of Sarsa and Expected Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSEUDOCODE: Double Q-Learning, for estimating Q_1 ~= Q_2 ~= q_*\n",
    "\"\"\"\n",
    "Algorithm parameters: step size alpha in (0,1], small eps > 0\n",
    "Initialize Q_1(s,a) and Q_2(s,a), for all s in S+, a in A(s), s.t. Q(terminal,.)=0\n",
    "\n",
    "Loop for each episode:\n",
    "    Initialize S\n",
    "    Loop for each step of episode:\n",
    "        Choose A from S using the policy eps-greedy in Q_1 + Q_2\n",
    "        Take action A, observe R, S'\n",
    "        With 0.5 probability:\n",
    "            Q_1(S,A) <- Q_1(S,A) + alpha(R + gamma*Q_2(S', argmax_a Q_1(S',a)) - Q_1(S,A))\n",
    "        else:\n",
    "            Q_2(S,A) <- Q_2(S,A) + alpha[R + gamma*Q_1(S', argmax_a Q_2(S',a)) - Q_2(S,A)]\n",
    "        S <- S'\n",
    "    until S is terminal\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.8 Games, Afterstates and Other Special Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A conventional state-value fxn evaluates states in which the agent has the option of selection an action\n",
    "- An afterstate value fxn evaluates the state after the agent has already acted\n",
    "    - Afterstates are useful when we have knowledge of an initial part of the environment's dynamics but not necessarily of the full dynamics.\n",
    "- Afterstate value fxns can produce more efficient learning than conventional action value fxns\n",
    "    - For example: Consider tic-tac-toe.  Many position-move pairs produce the same resulting position. A conventional action-value fxn would have to separately assess both pairs, whereas an afterstate value fxn would immediately assess both equally.\n",
    "    - Any learning about one position-move pair would immediately transfer to the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
