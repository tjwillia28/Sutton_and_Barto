{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7: n-step Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- n-step TD methods generalize both MC and one-step TD methods so that one can shift from one to the other smoothly as needed to meet the demands of a particular task\n",
    "- In many applications one wants to be able to update the action very fast to take into account anything that has changed, but boostrapping works best if it is over a length of time in which a significant and recognizable state change has occured.\n",
    "- n-step mehtods enable bootstrapping to occur over multiple steps, freeing us from the tyranny of hte single time step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.1 n-step TD Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MC methods perform an update for each state based on the entire sequence of observed rewards from that state until the end of the episode\n",
    "- The update of one-step TD methods is based on just the one next reward, bootstrapping from the value of the stae one step later as a proxy for the remaining rewards.\n",
    "- Methods that use n-step updates are still TD methods because they still change an earlier estimate based on how it differs from a later estimate. Now the later estimate is just n steps later.  Methods in which the temporal difference extends over n steps are called n-step TD methods\n",
    "- Terminology: $G_{t}$ is called the target of the update\n",
    "    - In MC updates the target is the total return over all time steps\n",
    "    - In one-step updates the target is the first reward plus the discounted estimated value of the next state, which is called the one-step return $$G_{t:t+1} \\doteq R_{t+1} + \\gamma V_{t}(S_{t+1})$$\n",
    "- The target for an arbitrary n-step update is the n-step return: $$ G_{t:t+n} \\doteq R_{t+1} + \\gamma R_{t+2} + ... + \\gamma^{n-1}R_{t+n} + \\gamma^{n}V_{t+n-1}(S_{t+n})$$ for all n, t s.t. n >= 1 and 0<= t<= T-n\n",
    "- All n-step returns can be considered approximations to the full return, truncated after n steps and then corrected for the remaining missing terms by $ V_{t+n-1}(S_{t+n}) $\n",
    "- The natural state-value learning algorithm for using n-step returns is thus: $$ V_{t+n}(S_{t}) \\doteq V_{t+n-1}(S_{t}) + \\alpha[G_{t:t+n} - V_{t+n-1}(S_{t})]$$ for 0 <= t <= T\n",
    "- Note that no changes at all are mae during the first n-1 steps because it has not seen the n states yet.  To make up for that, an equal number of additional updates are made at the end of the epidose, after termination and befrore starting the next episode\n",
    "- Error reduction property of n-step returns: an important property of n-step returns is that thir expectation is guaranteed to be a better estimate of $v_{\\pi}$ than $V_{t-n+!}$ is, in a worst-state sense. Theat is, the worst error of the expected n-step return is guaranteed to be less than or equal to $\\gamma^{n}$ time the worst error under $V_{t+n-1}$\n",
    "    - Because of the error reduction property, one can fomally show that all n-step TD methods converge to the correct predictions under appropriate technical conditions.\n",
    "- The generalization of TD and MC methods to n-step methods can potentially perform better than either of the two extreme methods\n",
    "    - n appears to be a hyperparamter that you tune "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSEUSOCODE: n-step TD for estimating V ~= v_PI\n",
    "\"\"\"\n",
    "Input:  a policy PI\n",
    "Algorithm paramaters: step size alpha in (0,1], a positive integer n\n",
    "Initialize V(s) arbitrarily, for all s in S\n",
    "All store and access operations (for S_t and R_t) can take their index mod n+!\n",
    "\n",
    "Loop for each episode:\n",
    "    Initialize and store S_0 /= terminal\n",
    "    T <- inf\n",
    "    Loop for t = 0,1,2,...:\n",
    "        If t < T, then:\n",
    "            Take an action according to PI(.|S_t)\n",
    "            Observe and store the reward as R_(t+1) and the next state as S_(t+1)\n",
    "            If S_(t+1) is terminal, then T <- t+1\n",
    "        tau <- t-n+1 (tau is the time whose state's estimate is being updated)\n",
    "        if tau >= 0:\n",
    "            G <- SUM_(i = tau + 1)->min(tau + n, T) gamma^(i-tau-1)R_i\n",
    "            If tau + n < T, then G <- G + gamma^n V(S_(tau+n))\n",
    "            V(S_tau) <- V(S_tau) + alpha[G - V(S_tau)]\n",
    "    Until tau = T-1\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.2 n-step Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The n-step version of Sarsa we call n-step Sarsa\n",
    "- The main idea is to switch states for actions(state-action pairs) and then use an epsilon-greedy policy\n",
    "- We redefine n-step returns(update targets) in terms of estimated action values: \n",
    "$$ G_{t:t+n} \\doteq R_{t+1} + \\gamma R_{t+2} + ... + \\gamma^{n-1}R_{t+n} + \\gamma^{n}Q_{t+n-1}(S_{t+n}, A_{t+n})$$ \n",
    "n >= 1, 0 <= t < T-n\n",
    "- The natural algorithm is then: \n",
    "$$Q_{t+n}(S_{t}, A_{t}) \\doteq Q_{t+n-1} (S_{t}, A_{t}) + \\alpha[G_{t:t+n} - Q_{t+n-1}(S_{t}, A_{t})]$$ 0 <= t <= T\n",
    "- n-step methods can speedup policy learning because n-step methods strengthens the last n action values of the sequence, so that much more is learned from the one episode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSEUDOCODE: n-step Sarsa for estimating Q~= q_* or q_PI\n",
    "\"\"\"\n",
    "Initialize Q(s,a) arbitrarily, for all s in S, a in A\n",
    "Initialize PI to be eps-greedy w.r.t. Q, or to a fixed given policy\n",
    "Algorithm parameters: step size alpha in (0,1], small eps > 0, a positive integer n\n",
    "All store and access operations(for S_t, A_t, R_t) can take their index mod n+1\n",
    "\n",
    "Loop for each episode:\n",
    "    Initialize and store S_0 /= terminal\n",
    "    Select and store an action A_0 ~ PI(.|S_0)\n",
    "    T <- inf\n",
    "    Loop for t = 0,1,2,...:\n",
    "        If t < T, then:\n",
    "            Take action A_t\n",
    "            Observe and store the next reward as R_(t+1) and the next state as S_(t+1)\n",
    "            If S_(t+1) is terminal, then:\n",
    "                T <- T+1\n",
    "            else:\n",
    "                Select and store an action A_{t+1} ~ PI(.|S_(t+1))\n",
    "        tau <- t - n+1\n",
    "        If tau >= 0:\n",
    "            G <- SUM_i=(tau+1)->min(tau+n, T) gamma^(i-tau-1) R_i\n",
    "            If tau +n < T, then G <- G + gamma^n(S_(tau+n), A_(tau+n))\n",
    "            Q(S_tau, A_tau) <- Q(S_tau, A_tau) + alpha[G - Q(S_tau, A_tau)]\n",
    "            If PI is being learned, then ensure that PI(.|S_tau) is eps-greedy w.r.t. Q\n",
    "        Until tau = T-1\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.3 n-step Off-policy Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recall that off-poliocy learning is learning the value function for one policy, $\\pi$, while following another policy, $b$. Often, $\\pi$ is the greedy policy for the current action-value function estimate, and $b$ is a more exploratory policy, perhaps eps-greedy.\n",
    "- In order to use the data from $b$ we must take into account the differences between the policies, using their relative probability of taking the actions that were taken.\n",
    "    - In n-step methods, returns are constructed over n steps, so we are interested in the relative probability of just those n actions\n",
    "- The off-policy version of n-step TD has the following update where $\\rho_{t:t+n-1}$ is the importancef-sampling ratio:\n",
    "$$ V_{t+n}(S_{t}) \\doteq V_{t+n-1}(S_{t}) + \\alpha \\rho_{t:t+n-1}[G_{t:t+n} - V_{t+n-1}(S_{t})]$$ 0 <= t<= T\n",
    "- The importance sampling ratio is the relative probability under the two policyes of taking the n actions from $A_{t}$  to $A_{t+n-1}$:\n",
    "$$ \\rho_{t:h} \\doteq \\prod\\limits_{k=t}^{min(h,T-1)} \\frac {\\pi(A_{k}|S_{k})} {b(A_{k}|S_{k})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rest of chapter is about a range of different n-step TD methods, we might be able to use eligibility traces instead..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
