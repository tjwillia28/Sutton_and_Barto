{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2: Policy Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our reason for computing the value function for a policy is to help find better policies.Suppose we have tetermined the value fxn $v_{\\pi}$ for an arbitrary deterministic policy $\\pi$.  For some state $s$ we would like to know whether or not we should change the policy to deterministically choose an action $a \\neq \\pi(s)$.  We know how good it is to follow the current policy from $s$- that is $v_{\\pi}$ but would it be better or worse to change to the new policy? One way to answer this is to consider selecing $a$ in $s$ and thereafter following, $\\pi$. The value of this way of behaving is $$q_{\\pi}(s, a) \\doteq E[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_{t} = s, A_{t} = a] = \\Sigma_{s', r} p(s',r|s,a)[r+\\gamma*v_{\\pi}(s')]$$\n",
    "\n",
    "- The key criterion is whether this is greater than or less that $v_{\\pi}(s)$.  If it is greater that is , if it is better to select a once in s and thereafter follow $\\pi$ than it would be better still to select a very time s is encountered, and that this new policy would in fact a better one overall.\n",
    "- The policy improvement theorem: Let $\\pi$ and $\\pi'$ be any pair of deterministiv policies s. t., $\\forall  s \\epsilon S$, $$ q_{\\pi}(s, \\pi(s))  \\ge v_{\\pi}(s)$$ \n",
    "\n",
    "- Then the policy $\\pi'$ must be as good as, or better than, $\\pi$. That is, it must obtain greater or equal expected return from all states $ s \\epsilon S$: $$v_{\\pi'}(s) \\geq v_{\\pi}(s)$$\n",
    "\n",
    "- So far we have seen how, given a policy and its value function, we can easlity evaluate a change in the policy at a single state to a particular action.  It is a natural extension to consider changes at all states and to all possible actions, selecting at each state the action that appears best according to $q_{\\pi}(s, a)$. In other words, to consider the new greedy policy , $\\pi'$, given by: $$\\pi'(s) \\doteq argmax_{a} q_{\\pi}(s, a) = argmax_{a} E[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_{t} = s, A_{t} = a] = argmax_{a} \\Sigma_{s',r}p(s',r|s,a)[r + \\gamma v_{\\pi}(s')]$$\n",
    "\n",
    "\n",
    "- The process of making a new policy that improves on an original policy, by making it greedy with respect to the value function of the original policy, is called policy improvement\n",
    "- Policy improvement must give us a stricly better policy except when the original policy is already optimal.\n",
    "- Although we only covered deterministic policies, the same ideas can be applied to stochastic policies\n",
    "    - if there are several acitons at which the maximum is svhieved, then in the stochastic case we wneed not select a single action from among them.  Instead, each maximizing action can be given a portion of the probability of being selected in the new greedy policy.  Any apportioning scheme is allowed as long as all submaximal actions are given zero probability.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3: Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Once a policy, $\\pi$, has been improved using $v_{\\pi}$ to yield a better policy, $\\pi'$, we can then compute $v_{\\pi'}$ and improve it again to yield an even better $\\pi''$\n",
    "- We can thus obtain a sequence of monotonically improving policies and value functions: $$\\pi_{0} \\rightarrow v_{\\pi_{0}} \\rightarrow \\pi_{1} \\rightarrow v_{\\pi_{1}} \\rightarrow ... \\rightarrow \\pi_{*} \\rightarrow v_{*}$$\n",
    "- We alternate evaluating the policy and improving the policy until the policy converges to the optimal policy\n",
    "- This way of finding an optimal policy is called policy iteration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSEUDOCODE\n",
    "'''\n",
    "Policy Iteration (using iterative policy evaluation) for estimating PI ~= PI_* \n",
    "\n",
    "1. Initialization\n",
    "    V(s) in R and PI(s) in A(s) arbitrarily for all s in S\n",
    "\n",
    "2. Policy Evaluation\n",
    "    Loop:\n",
    "        delta <- 0\n",
    "        Loop for each s in S:\n",
    "            v <- V(s)\n",
    "            V(s) <- SUM_s',r p(s', r|s, PI(s))[r+ gamma V(s')]\n",
    "            delta <- max(delta, |v-V(s)|)\n",
    "    until delta < theta (a small positive number determining the accuracy of estimation)\n",
    "\n",
    "3. Policy Improvement\n",
    "    policy-stable <- true\n",
    "    For each s in S:\n",
    "        old-action <- PI(s)\n",
    "        PI(s) <- argmax_a SUM_s',r p(s',r|s,a)[r + gamma V(s')]\n",
    "        If old-action /= PI(s), then policy-stable <- false\n",
    "    If policy-stable, then stop and return V ~= v_* and PI ~= PI_*; else go to 2\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4: Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One drawback to policy interation is that each of its iterations involves policy evaluation, which may itself be a protracted iterative computation requiring multiple sweeps through the state set.  If policy evaluation is done iteratively, then convergence exactly to $v_{\\pi}$ occurs only in the limit.  Must we wait for exact convergence, or can we stop short of that? We can stop policy evaluation short if additional iterations have no effect on the corresponding greedy policy.\n",
    "- The algorithm value iteration is when policy evaluation is stopped after just one sweep (one update of each state). It can be written as a particularly simple update operation that combines the policu improvement and truncated policy evaluation steps: $$ v_{k+1}(s) \\doteq max_{a} E[R_{t+1} + \\gamma v_{k}(S_{t+1}) | S_{t} = s, A_{t} = a] = max_{a} \\Sigma_{s',r} p(s', r|s, a) [r + \\gamma v_{k}(s')]$$ for all s $\\in S$.\n",
    "- For arbitraty $v_{0}$ the sequence ${v_{k}}$ can be shown to converge to $v_{*}$ under the same conditions that guarantee the existence of $v_{*}$\n",
    "- Note that the value iteration update is identical to the policy evaluation update except that it requires the maximum to be taken over all actions.\n",
    "- Value iteration ends in a similar way to policy evaluation, once the value function changes by only a small amount in a sweep.\n",
    "- Value iteration effectively combines, in each of its sweeps, one sweep of policy evaluation and one sweep of policy improvement\n",
    "- Faster convergence is often achieve by interposing multiple policy evaluation sweeps between each policy improvement sweep.  In general, the entire class of truncated policy iteration algorithms can be thought of as sequences of sweeps, some of which use policy evaluation updates and some of which use value iteration updates.  Since the max operation is the only difference between these updates, this just means that the max operation is added to some sweeps of policy evaluation.  All of these algorithms converge to an optimal policy for discounted finite MDPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSEUDOCODE\n",
    "\n",
    "'''Algorithm Parameter: a small therhold theta > 0 determining accuracy of estimation\n",
    "Initialize V(s), for all s in S+, arbitratily except that V(terminal) = 0\n",
    "\n",
    "Loop:\n",
    "    delta = 0\n",
    "    Loop for each s in S:\n",
    "        v <- V(s)\n",
    "        V(s) <- max_a SUM_s',r p(s', r|s, a)[r + gamma V(s')]\n",
    "        delta <- max(delta, |v - V(s)|)\n",
    "    until delta < theta\n",
    "    \n",
    "Output a determinisic policy, PI ~= PI_* s.t.\n",
    "PI(s) = argmax_a SUM_s',r p(s',r|s,a)[r+gammaV(s')]'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.5: Asynchronous Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Major drawback of the DP methods that we have discussed so far is that they involve operations over the entire state set of the MDP, that is, they require sweeps of the state set.  If the state set is very large, then even a single sweep can be prohibitely expensive.\n",
    "- Asynchronous DP algorithms are in-place iterative DP algorithms that are not organized in terms of systematic sweeps of the state set.  These algorithms update vales of states in any order whatsover, using whatever values of other states happen to be available.  The values of some states may be updated several times before the values of others are updated once.  To converge correctly, however, an asynchronous algoirhm must continue to update the values of all the staes: it can't ignore any state afte some point int he computation.\n",
    "- Asynchronous algorithms also make it easier to intermix computation with real-time interaction.  To solve a given MDP, we can run an iterative DP algorithm at the same time that an agent is actually experiencing the MDP.  The agent's exxperience can be used to determine the states to which the DP algorithm applies its updates.  At the same time, the lastest value and policy information from th DP algorithm can guide the agent's decision making.  For example, we can apply updates to staets as the agent visits them.  This makes it possible to focus the DP algorithm's updates onto parts of the state set that are most relevant to the agent.  This kind of focusing is a repeated theme in RL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.6: Generalied Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We use the term generalized policy iteration (GPI) to refer to the general idea of letting policy-evaluation and policy-improvement processes interact, independent of the granularity and other details of the two processes.\n",
    "- Note:\n",
    "    - Policy Evaluation: making the value fxn consistent with the current policy\n",
    "    - Policy Improvement: making the policy greedy w.r.t. the current value fxn\n",
    "    \n",
    "- If both the evaluation process and improvement process stablize, that is, no longer produce changes, then the value function and policy must be optimal.\n",
    "    - The value fxn stablizes only when it is consistent with the current policy\n",
    "    - The policy stablizes only when it is greedy w.r.t. the current value fxn\n",
    "- The two processes together achieve the overall goal of optimality even though neither is attempting to achive it directly\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
