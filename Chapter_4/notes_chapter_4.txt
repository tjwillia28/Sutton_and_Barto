### Sutton and Barto ###

*** Dynamic Programming ***

- The term dynamic programming refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as a MDP

- Classical DP algorithms are of limited utility both because fo their assumptions of a perfecr model and of their great computational expense, but they are still important theoretically

- All methods later in this book try to achieve the same effect as DP, only with less computation and without assuming a perfect model of the environment

- We usually assume that the environment is a finite MDP

- A common way of obtaining approximate solutions for tasks with continuos states and actions is to quantize the state and action spaces and then apply finite-state DP methods

- The key idea of DP, and of RL generally, is the use of value fxns to organize and structure the search for good policies

- We can easily obtain optimal policies once we have found the optimal value functions, v_* or q_*, which satisfy the Bellman optimality equations

- DP algorithms are obtained by turning Bellman equations such as these into assignments, that is, into update rules for improving approximations of the desired value functions.


** 4.1: Policy Evaluation (Prediction)

- Policy Evaluation: how to compute the state-value function v_PI for an arbitrary policy PI
	- This is also referred to as the prediction problem

- Iterative policy evaluation:
	- For our purposes, iterative solution methods are most suitable.  Consider a sequence of approximate value functions v_0, v_1, v_2, ..., each mapping S^+ to the real numbers.  The initial approximation, v_0, is chosen arbitrarily (exept that the terminal state, if any, must be given value 0), and each successive approximation is obtained using the Bellman equation for v_PI for all s element S_space.
	- The sequence {v_k} can be shown in general to converge to v_PI as k -> inf

	- Expected update operation:
		- To produce each successive approximation, v_(k+1) from v_k, iterative policy evaluation applies the same operation to each state s: it replaces the old value of s with a new value obtained from the old values of the successor states of s, and the expected immediate rewards, along all the one-step transitions possible under the policy being evaluated.
	- Each iteration of iterative policy evaluation updates the value of every state once to produce the new approximate value function v_(k+1)
	- The are several different kinds of expected updates, depending on whether a state or a state-action pair is being updates, and depending on the precies way the estimated values of the successor states are combined

- All updates done in DP algorithms are called expected updates because they are based on an expectation over all possible next states rather than on a sample next state

ITERATIVE POLICY EVALUATION PSEUDOCODE:
Input PI, the policy to be evaluated
Algorithm paramater: a small threshold theta > 0 determining accuracy of estimation
Initialize V(s), for all s element S^+, arbitrarily expect that V(terminal) = 0
Loop:
	delta <- 0
	Loop for each s element S^+:
		v <- V(s)
		V(s) <- SUM_a PI(a|s) SUM_s',r p(s',r|s,a)[r + gamma*V(s')]
		delta <- max(delta, |v - V(s)|)
	until delta < theta


** 4.2: Policy  

Our reason for computing the value function for a policy is to help find better policies.Suppose we have tetermined the value fxn  vπvπ  for an arbitrary deterministic policy  ππ . For some state  ss  we would like to know whether or not we should change the policy to deterministically choose an action  a≠π(s)a≠π(s) . We know how good it is to follow the current policy from  ss - that is  vπvπ  but would it be better or worse to change to the new policy? One way to answer this is to consider selecing  aa in  ss  and thereafter following,  ππ . The value of this way of behaving is
qπ(s,a)≐E[Rt+1+γvπ(St+1)|St=s,At=a]=Σs′,rp(s′,r|s,a)[r+γ∗vπ(s′)]
qπ(s,a)≐E[Rt+1+γvπ(St+1)|St=s,At=a]=Σs′,rp(s′,r|s,a)[r+γ∗vπ(s′)]
 
The key criterion is whether this is greater than or less that  vπ(s)vπ(s) . If it is greater that is , if it is better to select a once in s and thereafter follow  ππ  than it would be better still to select a very time s is encountered, and that this new policy would in fact a better one overall.

The policy improvement theorem: Let  ππ  and  π′π′  be any pair of deterministiv policies s. t.,  ∀sϵS∀sϵS ,
qπ(s,π(s))≥vπ(s)
qπ(s,π(s))≥vπ(s)
 
The process of making a new policy that improves on an original policy, by making it greedy with respect to the value function of the original policy, is called policy improvement
























