{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10: On-policy Control with Approximation  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will use a parametric approximation of the action-value function $\\hat{q}(s, a, \\textbf{w}) \\approx q_{*}(s,a)$, where $\\textbf{w} \\in \\mathbb{R}^{d}$ is a finite-dimensional weight vector\n",
    "- In the episodic case, the extension(i.e. using fxn approximation) is straightforward\n",
    " But in the continuing case, once we have function approximation we have to give up discounting and switch to a new \"average-reward\" formulation of the control problem, with ne \"differential\" value functions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.1 Episodic Semi-gradient Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The approximate action-value function, $\\hat{q} \\approx q_{\\pi}$, that is represented as a paramaterized functional form with weight vector **w**\n",
    "- We consider random training examples of the form $S_{t}, A_{t} \\rightarrow U_{t}$\n",
    "- The update target $U_{t}$ can be any approximation of $q_{\\pi}(S_{t}, A_{t})$ including the usual backed-up values\n",
    "- The general gradient-descent update for action-value prediction is:\n",
    "$$\\textbf{w}_{t+1} \\doteq \\textbf{w}_{t} + \\alpha[ U_{t} - \\hat{q}(S_{t}, A_{t}, \\textbf{w}_t)] \\nabla \\hat{q}(S_{t}, A_{t}, \\textbf{w}_{t})$$\n",
    "- Suitable techniques applicable to continuous actions, or to actions from large discrete sets, are a topic of ongoing research with as yet no clear resolution\n",
    "- On the other hand, if the action set is discrete and not too large, then we can use the techniques already developed in previous chapters\n",
    "    - That is, for each possible action *a* available in the current state $S_{t}$, we can compute $\\hat{q}(S_{t}, a \\textbf{w}_{t})$ and then find the greedy action $A_{t}^{*} = argmax_{a}\\hat{q}(S_{t}, a, \\textbf{w}_{t})$\n",
    "    - Policy improvement is then done (in the on-policy case) by changing the estimation policy to a soft approximation of the greedy policy suchas the eps-greedy policy\n",
    "    - Actions are selected according to this same policy\n",
    "- Linear funciton of approximation of action-value function given feature vectors $\\textbf{x}(s,a)$:\n",
    "$$\\hat{q}(s,a,\\textbf{w}) \\doteq \\textbf{w}^{T}\\textbf{x}(s,a) = \\sum_{i=1}^{d} w_{i} \\cdot x_{i}(s,a)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
