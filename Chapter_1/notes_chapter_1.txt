### Sutton and Barto ###

*** Chapter 1: Introduction

- We will explore a computational approach to learning from interaction
- We will explore idealized learning situations and evaluate the effectiveness of different learning methods

** 1.1 Reinforcement Learning

- Reinforcement learning is learning what to do - how to map situations to actions- so as to maximize a numerical reward signal

- Trial-and-error Search: The learner is not told which actions to take.  Rather, it must discover which actions yield the most reward by trying them.

- Delayed Rewards: Actions affect not only the immediate reward but also the next situation and subsequent rewards.

- Reinforcement Learning is three things:
	1. a problem
	2. a class of solution methods
	3. a field that studies 1 and 2

- The RL problem is formalized using incompletely-known Markov Decision Processes
	- Capture the most important aspects of the real problem facing a learning agent interacting over time with its environment to achieve a goal
	- Markov Decision Processes must include:
		1. Sensation
		2. Action
		3. Goal

- Exploration-Exploitation Dilemma:
	- The agent has to exploit what it has already experienced in order to obtain reward, but it also has to explore in order to make better action selections in the future
	- The dilemma is that neither exploration nor exploitation can be pursued exclusively without failing at the task
	- In stochastic tasks, each action must be tried many times to gain a reliable estimate of its expected reward
	- The exploration-exploitation dilemma remains unsolved

- It is usually assumed from the beginning that the agent has to operate despite significant uncertainty about the environment

- When RL involves planning, it has to address the interplay between planning and real-time action selection, as well as how environment models are acquired and improved.

- When RL uses supervised learning, it does so for specific reasons that determine which capabilities are critical and which are not

- An RL agent does not always mean something like a complete organism or robot. An RL agent can also be a component of a larger behaving system


** 1.2 Examples

- All examples involve interaction between an active decsion making agent and its environment, within which the agent seeks to maximize a goal despite uncertainty about its environment

- In all examples the effects of actions cannot be fully predicted; thus the agent must monitor its environment frequently and react appropriately

- In all examples the agent can use its experience to improve its perfomance over time. The knowledge the agent brings to the task at the start- either from previous experience with related tasks or built into it by design or evolution- influences what is useful or easy to learn, but interaction with the environment is essential for adjusting behavior to exploit specific features of the task


** 1.3: Elements of Reinforcment Learning

 - Beyond the agent and environment, there are four main subelements of a RL system:
 	1. Policy
 	2. Reward Signal
 	3. Value Function
 	4. Model (optional)

 - Policy:
 	- A policy defines the learning agent's way of behaving at a given time
 	- It is a mapping from perceived states of the environment to actions to be taken in those states
 	- In some cases, a policy can be a simple fxn or lookup table, whereas in others it may involve extensive computation such as a search process
 	- The policy is the core of a RL agent in the sense that it alone is sufficient to determine behavior
 	- In general, policies may be stochastic, specifying probabilities for each action

 - Reward Signal:
 	- Defines the goal of a RL problem
 	- Defines what is good in an immediate sense
 	- On each time step, the environment sends to the agent a single number called the reward
 	- The agent's sole objective is to maximize the total reward it recieves over the long run
 	- The reward signal defines what are good and bad events for the agent
 	- The reward signal is the primary basis for altering the policy
 		- If an action selected by the policy is followed by a low reward, then the policy may be changed to select some other action in that situation in the future
 	- In general, reward signals may be stochastic fxns of the state of the environment and the actions taken

 - Value Function:
 	- Defines what is good in the long run
 	- The value of a state is the total amount of reward an agent can expect to accumulate over the future, starting from that state
 	- Indicate the long-term desirability of states after taking into account the states that are likely to follow and the rewards available in those states

- Model:
	- Model of the environment
	- A model either mimics the behavior of the environment or allows inferences to be made about how the environment will behave
	- Models are used for planning, by which we mean any way of deciding on a course of action by considering possible future situations before they are actually experienced.
	- Methods for solving RL problems that use models and planning are called model-based methods, as opposed to simpler model-free methods that are explicitly trial-and-error learners


- Rewards are in a sense primary, whereas values, as predictions of rewards, are secondary.  Without rewards there could be no values, and the only purpose of estimating values is to achieve more reward.  Nevertheless, it is values with which we are most concerned when making and evaluating decsions.  Action choices made based on value judgments. We seek actions that bring about states with the highest value, not highest reward, because these actions obtain the greatest amount of reward for us over the long run.  Unfortunately, it is much harder to determine values than it is to determine rewards. Rewards are basically given directly by the environment, but values must be estimated and re-estimated from the sequences of observations an agent makes over its entire lifetime.


** 1.4: Limitations and Scope

- RL relies heavily on the concept of state- as input to the policy and value fxn, and as both input to and output from the model.

- Informally, we can think of the state as a signal conveying to the agent some sense of 'how the environment is' at a particular time. i.e. whatever information is available to the agent about its environment.

- Evolutionary methods are NOT covered in this book


** 1.5: An Extended Example: Tic-Tac-Toe

- How a tick-tac-toe problem would be approached with a method making use of a value function:
	1. Set up a table of numbers, one for each possible state of the game. Each number will be latest estimate of the probability of our winning from that state.  We treat this estimate as the state's value, and the whole table is the learned value fxn.
		- State A has higher value than State B if the current estimate of the probability of winning from A is higher than it is from B
		- Assuming we are Xs, states with three Xs in a row have probability of winning = 1 and states with 3 Os in a row have probability of winning = 0.  All other states are initialized with P(winning) = 0.5
	
	2. We then play many games against the opponent.  To select our moves we examine the states that would result from each of our possible moves and look up their current values in the table.  Most of the time we move greedily(i.e. select the move with the highest value).  Occasionally, however, we select randomly from among the other moves instead.  These are called exploratory moves because they cause us to experience states that we might otherwise never see.
	
	3. While we are playing, we change the values of the states in which we find ourselves during the game.  We attempt to make them more accurate estimates of the probabilities of winning. To do this, we "back up" the value of the state after each greedy move to the state before the move.  More precisely, the current value of the earlier state is updated to be closer to the value of the later state.  This can be done by moving the earlier state's value a fraction of the way toward the value of the later state.
		- Let S_t denote the state before the greedy move, and S_(t+1) denote the state after the move, and alpha be the learning rate, then the update to the estimated value of S_t, denoted V(S_t) can be written as:
			V(S_t) <- V(S_t) + alpha[V(S_(t+1) - V(S_t))]

	- The method above performs well on the task.  If the step-size parameter is reduced properly over time, then this method converges, for any fixed oppoent, to the true probabilities of winning from each state given optimal play by our agent.  Furthermore, the moves then taken(except on explotatory moves) are in fact the optimal moves against this (imperfect) opponent.  In other words, the method converges to an optimal policy for playing the game against this opponent.  If the step-size parameter is not reduced all the way to zero over time, then this player also plays well against opponents that slowly change their way of playing.


- Temporal-Difference Learning Method: a learning method whose changes are based on a difference between estimates at two successive times.(i.e. V(S_(t+1)) - V(S_t))

- RL is not restricted to problems in which the behavior breaks down into separate episodes and can be used when behavior continues indefinitely

- RL can be used in discrete-time problems or continuous-time problems

- RL can be used when the state set is very large, or even infinite
	- With this many states it is impossible ever to experience more than a small fraction of them
	- Can use NN to provide the program with the ability to generalize from its experience, so that in new states it selects moves based on information saved from similar states faced in the past, as determined by its network.  How well a reinforcement learning system can work in problems with such large state sets is intimately tied to how appropriately it can generalize from past experience.
		- NNs aren't always the best way of generalizing

- RL can be used when part of the state is hidden, or when different states appear to the learner to be the same













